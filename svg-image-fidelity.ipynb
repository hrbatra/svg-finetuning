{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede4b6fa",
   "metadata": {
    "_cell_guid": "4cf02a6f-b7e9-4360-892d-b1a50793eb12",
    "_uuid": "2a1239f3-55fc-4dbb-9d3a-e90bfffa038c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-07T17:37:13.203696Z",
     "iopub.status.busy": "2025-03-07T17:37:13.203360Z",
     "iopub.status.idle": "2025-03-07T17:37:36.853066Z",
     "shell.execute_reply": "2025-03-07T17:37:36.852140Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 23.656547,
     "end_time": "2025-03-07T17:37:36.854652",
     "exception": false,
     "start_time": "2025-03-07T17:37:13.198105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from math import prod\n",
    "from statistics import mean\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "import cairosvg\n",
    "import clip\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "\n",
    "svg_constraints = kagglehub.package_import('metric/svg-constraints')\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str\n",
    ") -> float:\n",
    "    \"\"\"Calculates a fidelity score by comparing generated SVG images to target text descriptions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : pd.DataFrame\n",
    "        A DataFrame containing target text descriptions. Must have a column named 'description'.\n",
    "    submission : pd.DataFrame\n",
    "        A DataFrame containing generated SVG strings. Must have a column named 'svg'.\n",
    "    row_id_column_name : str\n",
    "        The name of the column containing row identifiers. This column is removed before scoring.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean fidelity score (a value between 0 and 1) representing the average similarity between the generated SVGs and their descriptions.\n",
    "        A higher score indicates better fidelity.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the 'svg' column in the submission DataFrame is not of string type or if validation of the SVG fails.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'description': ['red ball', 'swimming pool']\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'svg': ['<svg viewBox=\"0 0 100 100\"><circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\"/></svg>',\n",
    "    ...         '<svg viewBox=\"0 0 100 100\"><rect x=\"10\" y=\"10\" width=\"80\" height=\"80\" fill=\"blue\"/></svg>']\n",
    "    ... })\n",
    "    >>> score(solution, submission, 'id')\n",
    "    0...\n",
    "    \"\"\"\n",
    "    # Validate\n",
    "    del solution[row_id_column_name], submission[row_id_column_name]\n",
    "    if not pd.api.types.is_string_dtype(submission.loc[:, 'svg']):\n",
    "        raise ParticipantVisibleError('svg must be a string.')\n",
    "    # check that SVG code meets defined constraints\n",
    "    constraints = svg_constraints.SVGConstraints()\n",
    "    try:\n",
    "        for svg in submission.loc[:, 'svg']:\n",
    "            constraints.validate_svg(svg)\n",
    "    except:\n",
    "        raise ParticipantVisibleError('SVG code violates constraints.')\n",
    "\n",
    "    # Score\n",
    "    vqa_evaluator = VQAEvaluator()\n",
    "    aesthetic_evaluator = AestheticEvaluator()\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        for svg, description in zip(\n",
    "            submission.loc[:, 'svg'], solution.loc[:, 'description'], strict=True\n",
    "        ):\n",
    "            image = svg_to_png(svg)\n",
    "            vqa_score = vqa_evaluator.score(image, 'SVG illustration of ' + description)\n",
    "            aesthetic_score = aesthetic_evaluator.score(image)\n",
    "            instance_score = harmonic_mean(vqa_score, aesthetic_score, beta=2.0)\n",
    "            results.append(instance_score)\n",
    "\n",
    "    except:\n",
    "        raise ParticipantVisibleError('SVG failed to score.')\n",
    "\n",
    "    fidelity = mean(results)\n",
    "    return float(fidelity)\n",
    "\n",
    "\n",
    "class VQAEvaluator:\n",
    "    \"\"\"Evaluates images based on their similarity to a given text description.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        self.model_path = kagglehub.model_download(\n",
    "            'google/paligemma-2/transformers/paligemma2-10b-mix-448'\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_path)\n",
    "        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            self.model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=self.quantization_config,\n",
    "        )\n",
    "        self.questions = {\n",
    "            'fidelity': 'Does <image> portray \"{}\" without any lettering? Answer yes or no.',\n",
    "            'text': '<image> Text present: yes or no?',\n",
    "        }\n",
    "\n",
    "    def score(self, image: Image.Image, description: str) -> float:\n",
    "        \"\"\"Evaluates the fidelity of an image to a target description using VQA yes/no probabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : PIL.Image.Image\n",
    "            The image to evaluate.\n",
    "        description : str\n",
    "            The text description that the image should represent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The score (a value between 0 and 1) representing the match between the image and its description.\n",
    "        \"\"\"\n",
    "        p_fidelity = self.get_yes_probability(image, self.questions['fidelity'].format(description))\n",
    "        p_text = self.get_yes_probability(image, self.questions['text'])\n",
    "        return p_fidelity * (1 - p_text)\n",
    "\n",
    "    def mask_yes_no(self, logits):\n",
    "        \"\"\"Masks logits for 'yes' or 'no'.\"\"\"\n",
    "        yes_token_id = self.processor.tokenizer.convert_tokens_to_ids('yes')\n",
    "        no_token_id = self.processor.tokenizer.convert_tokens_to_ids('no')\n",
    "        yes_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' yes')\n",
    "        no_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' no')\n",
    "\n",
    "        mask = torch.full_like(logits, float('-inf'))\n",
    "        mask[:, yes_token_id] = logits[:, yes_token_id]\n",
    "        mask[:, no_token_id] = logits[:, no_token_id]\n",
    "        mask[:, yes_with_space_token_id] = logits[:, yes_with_space_token_id]\n",
    "        mask[:, no_with_space_token_id] = logits[:, no_with_space_token_id]\n",
    "        return mask\n",
    "\n",
    "    def get_yes_probability(self, image, prompt) -> float:\n",
    "        inputs = self.processor(images=image, text=prompt, return_tensors='pt').to(\n",
    "            'cuda:0'\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]  # Logits for the last (predicted) token\n",
    "            masked_logits = self.mask_yes_no(logits)\n",
    "            probabilities = torch.softmax(masked_logits, dim=-1)\n",
    "\n",
    "        yes_token_id = self.processor.tokenizer.convert_tokens_to_ids('yes')\n",
    "        no_token_id = self.processor.tokenizer.convert_tokens_to_ids('no')\n",
    "        yes_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' yes')\n",
    "        no_with_space_token_id = self.processor.tokenizer.convert_tokens_to_ids(' no')\n",
    "\n",
    "        prob_yes = probabilities[0, yes_token_id].item()\n",
    "        prob_no = probabilities[0, no_token_id].item()\n",
    "        prob_yes_space = probabilities[0, yes_with_space_token_id].item()\n",
    "        prob_no_space = probabilities[0, no_with_space_token_id].item()\n",
    "\n",
    "        total_yes_prob = prob_yes + prob_yes_space\n",
    "        total_no_prob = prob_no + prob_no_space\n",
    "\n",
    "        total_prob = total_yes_prob + total_no_prob\n",
    "        renormalized_yes_prob = total_yes_prob / total_prob\n",
    "\n",
    "        return renormalized_yes_prob\n",
    "\n",
    "\n",
    "class AestheticPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class AestheticEvaluator:\n",
    "    def __init__(self):\n",
    "        self.model_path = '/kaggle/input/sac-logos-ava1-l14-linearmse/sac+logos+ava1-l14-linearMSE.pth'\n",
    "        self.clip_model_path = '/kaggle/input/openai-clip-vit-large-patch14/ViT-L-14.pt'\n",
    "        self.predictor, self.clip_model, self.preprocessor = self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Loads the aesthetic predictor model and CLIP model.\"\"\"\n",
    "        state_dict = torch.load(self.model_path, weights_only=True, map_location='cuda:1')\n",
    "\n",
    "        # CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "        predictor = AestheticPredictor(768)\n",
    "        predictor.load_state_dict(state_dict)\n",
    "        predictor.to('cuda:1')\n",
    "        predictor.eval()\n",
    "        clip_model, preprocessor = clip.load(self.clip_model_path, device='cuda:1')\n",
    "\n",
    "        return predictor, clip_model, preprocessor\n",
    "\n",
    "\n",
    "    def score(self, image: Image.Image) -> float:\n",
    "        \"\"\"Predicts the CLIP aesthetic score of an image.\"\"\"\n",
    "        image = self.preprocessor(image).unsqueeze(0).to('cuda:1')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image)\n",
    "            # l2 normalize\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            image_features = image_features.cpu().detach().numpy()\n",
    "\n",
    "        score = self.predictor(torch.from_numpy(image_features).to('cuda:1').float())\n",
    "\n",
    "        return score.item() / 10.0  # scale to [0, 1]\n",
    "\n",
    "\n",
    "def harmonic_mean(a: float, b: float, beta: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the harmonic mean of two values, weighted using a beta parameter.\n",
    "\n",
    "    Args:\n",
    "        a: First value (e.g., precision)\n",
    "        b: Second value (e.g., recall)\n",
    "        beta: Weighting parameter\n",
    "\n",
    "    Returns:\n",
    "        Weighted harmonic mean\n",
    "    \"\"\"\n",
    "    # Handle zero values to prevent division by zero\n",
    "    if a <= 0 or b <= 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * (a * b) / (beta**2 * a + b)\n",
    "\n",
    "\n",
    "def svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an SVG string to a PNG image using CairoSVG.\n",
    "\n",
    "    If the SVG does not define a `viewBox`, it will add one using the provided size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    svg_code : str\n",
    "        The SVG string to convert.\n",
    "    size : tuple[int, int], default=(384, 384)\n",
    "        The desired size of the output PNG image (width, height).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "        The generated PNG image.\n",
    "    \"\"\"\n",
    "    # Ensure SVG has proper size attributes\n",
    "    if 'viewBox' not in svg_code:\n",
    "        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n",
    "\n",
    "    # Convert SVG to PNG\n",
    "    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n",
    "    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 224423433,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 226192131,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 226327421,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164716,
     "modelInstanceId": 225001,
     "sourceId": 263093,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.372103,
   "end_time": "2025-03-07T17:37:39.448051",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-07T17:37:11.075948",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
